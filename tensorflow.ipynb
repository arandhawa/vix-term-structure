{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Getting Started With TensorFlow\n",
    "\n",
    "<https://www.tensorflow.org/get_started/get_started>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_6:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Const_7:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0, dtype=tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicitly\n",
    "print(node1, node2, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run([node1, node2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node3:  Tensor(\"Add_3:0\", shape=(), dtype=float32)\n",
      "sess.run(node3):  7.0\n"
     ]
    }
   ],
   "source": [
    "node3 = tf.add(node1, node2) # Seems like you can also write node1 + node2 instead\n",
    "print(\"node3: \", node3)\n",
    "print(\"sess.run(node3): \",sess.run(node3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[ 3.  7.]\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b  # + provides a shortcut for tf.add(a, b)\n",
    "print(sess.run(adder_node, {a: 3, b:4.5}))\n",
    "print(sess.run(adder_node, {a: [1,3], b: [2, 4]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.5\n"
     ]
    }
   ],
   "source": [
    "add_and_triple = adder_node * 3.\n",
    "print(sess.run(add_and_triple, {a: 3, b:4.5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.30000001  0.60000002  0.90000004]\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable([.3], dtype=tf.float32) # Variables need type and initial value\n",
    "b = tf.Variable([-.3], dtype=tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "print(sess.run(linear_model, {x:[1,2,3,4]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.66\n"
     ]
    }
   ],
   "source": [
    "y = tf.placeholder(tf.float32)\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "fixW = tf.assign(W, [-1.]) # Assign W and b manually its perfect values\n",
    "fixb = tf.assign(b, [1.])\n",
    "sess.run([fixW, fixb])\n",
    "print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.train API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "sess.run(init) # reset values to incorrect defaults.\n",
    "for i in range(1000):\n",
    "    sess.run(train, {x:[1,2,3,4], y:[0,-1,-2,-3]})\n",
    "print(sess.run([W, b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11\n"
     ]
    }
   ],
   "source": [
    "curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x:[1,2,3,4], y:[0,-1,-2,-3]})\n",
    "print(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.contrib.learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp64kh60zp\n",
      "INFO:tensorflow:Using config: {'_task_id': 0, '_environment': 'local', '_session_config': None, '_num_ps_replicas': 0, '_master': '', '_model_dir': '/tmp/tmp64kh60zp', '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_task_type': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd15ddf1e10>, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_is_chief': True, '_keep_checkpoint_max': 5, '_save_summary_steps': 100, '_num_worker_replicas': 0}\n",
      "WARNING:tensorflow:From /home/leyht/bachelorthesis/vix-term-structure/venv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmp64kh60zp/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 4.75\n",
      "INFO:tensorflow:global_step/sec: 1009.39\n",
      "INFO:tensorflow:step = 101, loss = 0.0810971 (0.102 sec)\n",
      "INFO:tensorflow:global_step/sec: 1059.68\n",
      "INFO:tensorflow:step = 201, loss = 0.0137664 (0.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 987.139\n",
      "INFO:tensorflow:step = 301, loss = 0.00101854 (0.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 988.281\n",
      "INFO:tensorflow:step = 401, loss = 0.000945688 (0.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 976.673\n",
      "INFO:tensorflow:step = 501, loss = 0.000179357 (0.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 955.943\n",
      "INFO:tensorflow:step = 601, loss = 7.44669e-05 (0.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 962.168\n",
      "INFO:tensorflow:step = 701, loss = 2.03111e-05 (0.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 1131.05\n",
      "INFO:tensorflow:step = 801, loss = 5.28096e-06 (0.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 1282.53\n",
      "INFO:tensorflow:step = 901, loss = 5.24026e-07 (0.078 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmp64kh60zp/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.84218e-07.\n",
      "WARNING:tensorflow:From /home/leyht/bachelorthesis/vix-term-structure/venv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-14:40:06\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp64kh60zp/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-14:40:06\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 2.47523e-07\n",
      "WARNING:tensorflow:From /home/leyht/bachelorthesis/vix-term-structure/venv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-14:40:07\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp64kh60zp/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-14:40:07\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 0.00256144\n",
      "train loss: {'global_step': 1000, 'loss': 2.4752279e-07}\n",
      "eval loss: {'global_step': 1000, 'loss': 0.0025614444}\n"
     ]
    }
   ],
   "source": [
    "# Declare list of features. We only have one real-valued feature. There are many\n",
    "# other types of columns that are more complicated and useful.\n",
    "features = [tf.contrib.layers.real_valued_column(\"x\", dimension=1)]\n",
    "# An estimator is the front end to invoke training (fitting) and evaluation\n",
    "# (inference). There are many predefined types like linear regression,\n",
    "# logistic regression, linear classification, logistic classification, and\n",
    "# many neural network classifiers and regressors. The following code\n",
    "# provides an estimator that does linear regression.\n",
    "estimator = tf.contrib.learn.LinearRegressor(feature_columns=features)\n",
    "# TensorFlow provides many helper methods to read and set up data sets.\n",
    "# Here we use two data sets: one for training and one for evaluation\n",
    "# We have to tell the function how many batches\n",
    "# of data (num_epochs) we want and how big each batch should be.\n",
    "x_train = np.array([1., 2., 3., 4.])\n",
    "y_train = np.array([0., -1., -2., -3.])\n",
    "x_eval = np.array([2., 5., 8., 1.])\n",
    "y_eval = np.array([-1.01, -4.1, -7, 0.])\n",
    "input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\":x_train}, y_train,\n",
    "                                              batch_size=4,\n",
    "                                              num_epochs=1000)\n",
    "eval_input_fn = tf.contrib.learn.io.numpy_input_fn(\n",
    "    {\"x\":x_eval}, y_eval, batch_size=4, num_epochs=1000)\n",
    "# We can invoke 1000 training steps by invoking the  method and passing the\n",
    "# training data set.\n",
    "estimator.fit(input_fn=input_fn, steps=1000)\n",
    "# Here we evaluate how well our model did.\n",
    "train_loss = estimator.evaluate(input_fn=input_fn)\n",
    "eval_loss = estimator.evaluate(input_fn=eval_input_fn)\n",
    "print(\"train loss: %r\"% train_loss)\n",
    "print(\"eval loss: %r\"% eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpgeo7ycmy\n",
      "INFO:tensorflow:Using config: {'_task_id': 0, '_environment': 'local', '_session_config': None, '_num_ps_replicas': 0, '_master': '', '_model_dir': '/tmp/tmpgeo7ycmy', '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_task_type': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd12fbc7908>, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_is_chief': True, '_keep_checkpoint_max': 5, '_save_summary_steps': 100, '_num_worker_replicas': 0}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpgeo7ycmy/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 190.698598493\n",
      "INFO:tensorflow:global_step/sec: 980.317\n",
      "INFO:tensorflow:step = 101, loss = 0.0696153591903 (0.106 sec)\n",
      "INFO:tensorflow:global_step/sec: 1012.52\n",
      "INFO:tensorflow:step = 201, loss = 0.00274743119205 (0.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 913.348\n",
      "INFO:tensorflow:step = 301, loss = 0.000751497566326 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 952.13\n",
      "INFO:tensorflow:step = 401, loss = 5.58683933656e-05 (0.106 sec)\n",
      "INFO:tensorflow:global_step/sec: 973.039\n",
      "INFO:tensorflow:step = 501, loss = 3.3248942859e-06 (0.102 sec)\n",
      "INFO:tensorflow:global_step/sec: 904.649\n",
      "INFO:tensorflow:step = 601, loss = 7.28128620586e-07 (0.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 875.711\n",
      "INFO:tensorflow:step = 701, loss = 4.80061111939e-08 (0.115 sec)\n",
      "INFO:tensorflow:global_step/sec: 1051.59\n",
      "INFO:tensorflow:step = 801, loss = 5.17788187663e-09 (0.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 1191.6\n",
      "INFO:tensorflow:step = 901, loss = 1.8531679685e-10 (0.084 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmpgeo7ycmy/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.99836539169e-11.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-17:52:52\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpgeo7ycmy/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-17:52:53\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 2.23792e-11\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-17:52:53\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpgeo7ycmy/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-17:52:54\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 0.0101007\n",
      "train loss: {'global_step': 1000, 'loss': 2.2379192e-11}\n",
      "eval loss: {'global_step': 1000, 'loss': 0.010100702}\n"
     ]
    }
   ],
   "source": [
    "# Declare list of features, we only have one real-valued feature\n",
    "def model(features, labels, mode):\n",
    "    # Build a linear model and predict values\n",
    "    W = tf.get_variable(\"W\", [1], dtype=tf.float64)\n",
    "    b = tf.get_variable(\"b\", [1], dtype=tf.float64)\n",
    "    y = W*features['x'] + b\n",
    "    # Loss sub-graph\n",
    "    loss = tf.reduce_sum(tf.square(y - labels))\n",
    "    # Training sub-graph\n",
    "    global_step = tf.train.get_global_step()\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "    train = tf.group(optimizer.minimize(loss),\n",
    "                     tf.assign_add(global_step, 1))\n",
    "    # ModelFnOps connects subgraphs we built to the\n",
    "    # appropriate functionality.\n",
    "    return tf.contrib.learn.ModelFnOps(\n",
    "        mode=mode, predictions=y,\n",
    "        loss=loss,\n",
    "        train_op=train)\n",
    "estimator = tf.contrib.learn.Estimator(model_fn=model)\n",
    "# define our data sets\n",
    "x_train = np.array([1., 2., 3., 4.])\n",
    "y_train = np.array([0., -1., -2., -3.])\n",
    "x_eval = np.array([2., 5., 8., 1.])\n",
    "y_eval = np.array([-1.01, -4.1, -7, 0.])\n",
    "input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\": x_train}, y_train, 4, num_epochs=1000)\n",
    "# train\n",
    "estimator.fit(input_fn=input_fn, steps=1000)\n",
    "# Here we evaluate how well our model did. \n",
    "train_loss = estimator.evaluate(input_fn=input_fn)\n",
    "eval_loss = estimator.evaluate(input_fn=eval_input_fn)\n",
    "print(\"train loss: %r\"% train_loss)\n",
    "print(\"eval loss: %r\"% eval_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep MNIST for Experts\n",
    "\n",
    "<https://www.tensorflow.org/get_started/mnist/pros>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Softmax Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instead you could have used an interactive session here but these are boring.\n",
    "sess = tf.Session()\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.matmul(x,W) + b\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "for _ in range(1000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, {x: batch[0], y_: batch[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91949999"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(accuracy, {x: mnist.test.images, y_: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Multilayer Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.06\n",
      "step 100, training accuracy 0.86\n",
      "step 200, training accuracy 0.76\n",
      "step 300, training accuracy 0.9\n",
      "step 400, training accuracy 0.94\n",
      "step 500, training accuracy 0.96\n",
      "step 600, training accuracy 0.94\n",
      "step 700, training accuracy 0.98\n",
      "step 800, training accuracy 0.98\n",
      "step 900, training accuracy 1\n",
      "step 1000, training accuracy 0.96\n",
      "step 1100, training accuracy 0.92\n",
      "step 1200, training accuracy 0.98\n",
      "step 1300, training accuracy 0.94\n",
      "step 1400, training accuracy 0.94\n",
      "step 1500, training accuracy 0.94\n",
      "step 1600, training accuracy 0.98\n",
      "step 1700, training accuracy 0.94\n",
      "step 1800, training accuracy 0.96\n",
      "step 1900, training accuracy 1\n",
      "test accuracy 0.9753\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(2000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = sess.run(accuracy, {x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "            print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "        sess.run(train_step, {x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    print('test accuracy %g' % sess.run(accuracy, {x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
